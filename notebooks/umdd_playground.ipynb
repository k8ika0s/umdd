{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# UMDD Notebook Playground\n",
        "Hands-on tour for UMDD: synth data, heuristic preview, train a tiny multi-head model (codepage + tags + boundaries), and run inference. The goal is to give notebook users a single place to explore without touching the CLI."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prereqs\n",
        "* Run this notebook from the repo root.\n",
        "* Ensure deps are installed (one-time): `pip install -e '.[dev]'`.\n",
        "* Torch CPU is sufficient for these tiny demos; GPU is optional."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "ROOT = Path.cwd()\n",
        "if (ROOT / 'pyproject.toml').exists():\n",
        "    sys.path.append(str(ROOT))\n",
        "print('Project root:', ROOT)\n",
        "print('Pyproject exists:', (ROOT / 'pyproject.toml').exists())\n",
        "print('Python path contains root:', str(ROOT) in sys.path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate a small synthetic RDW dataset + metadata for experimentation.\n",
        "from umdd.data.generator import generate_synthetic_dataset\n",
        "import orjson\n",
        "\n",
        "data, meta = generate_synthetic_dataset(count=4, seed=42)\n",
        "Path('data').mkdir(exist_ok=True)\n",
        "Path('data/notebook_synth.bin').write_bytes(data)\n",
        "Path('data/notebook_synth.json').write_bytes(orjson.dumps(meta, option=orjson.OPT_INDENT_2))\n",
        "print('Bytes:', len(data))\n",
        "print('Records:', len(meta))\n",
        "meta[:2]  # preview first few metadata entries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Heuristic decode preview (useful before models are trained).\n",
        "from umdd.decoder import heuristic_decode\n",
        "\n",
        "heuristic_summary = heuristic_decode(data, preview_bytes=128)\n",
        "heuristic_summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train a tiny multi-head model on synthetic data (fast CPU run).\n",
        "from umdd.training.multitask import MultiTaskConfig, train_multitask\n",
        "\n",
        "cfg = MultiTaskConfig(\n",
        "    output_dir=Path('artifacts/notebook-multihead'),\n",
        "    samples_per_codepage=8,\n",
        "    max_len=96,\n",
        "    batch_size=4,\n",
        "    epochs=1,\n",
        "    embed_dim=32,\n",
        "    num_heads=2,\n",
        "    num_layers=1,\n",
        "    ff_dim=64,\n",
        "    device='cpu',\n",
        ")\n",
        "metrics = train_multitask(cfg)\n",
        "metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run inference on the synthetic bytes using the trained checkpoint.\n",
        "from umdd.inference import infer_bytes\n",
        "\n",
        "results = infer_bytes(data, checkpoint=Path(metrics['checkpoint']), max_records=2)\n",
        "for r in results:\n",
        "    print(f'Record {r.record_index} (len={r.byte_length})')\n",
        "    print('  Codepage:', r.codepage, 'conf', round(r.codepage_confidence, 3))\n",
        "    print('  Tag spans:', r.tag_spans)\n",
        "    print('  Boundary positions:', r.boundary_positions)\n",
        "    print('---')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inference outputs (JSONL/Arrow)\n",
        "Demonstrate writing inference results to JSONL and Arrow IPC for downstream tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from umdd.inference import results_to_jsonl, results_to_arrow\n",
        "import pandas as pd\n",
        "import pyarrow.ipc as pa_ipc\n",
        "\n",
        "logs_dir = Path('logs')\n",
        "logs_dir.mkdir(exist_ok=True)\n",
        "jsonl_path = logs_dir / 'notebook_infer.jsonl'\n",
        "arrow_path = logs_dir / 'notebook_infer.arrow'\n",
        "\n",
        "results_to_jsonl(results, jsonl_path)\n",
        "results_to_arrow(results, arrow_path)\n",
        "print('Wrote', jsonl_path, 'and', arrow_path)\n",
        "pd.read_json(jsonl_path, lines=True).head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with pa_ipc.open_file(arrow_path) as reader:\n",
        "    table = reader.read_all()\n",
        "table.to_pandas().head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notes\n",
        "* These runs stay tiny for speed; bump `samples_per_codepage`, `epochs`, or `embed_dim` if you have more time/resources.\n",
        "* Swap `data/notebook_synth.bin` with a real RDW dataset to see how the model behaves on authentic bytes.\n",
        "* CLI equivalents exist (`umdd dataset synthetic`, `umdd train multitask`, `umdd infer`) if you prefer terminal workflows."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}